# -*- coding: utf-8 -*-
"""CNN Model 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10RbDfNjsmYL6rbQ87JbXZO6r6-hXg601
"""

COVID19_data = "/content/drive/MyDrive/CNN/COVID 19/Covid 19"
COVID19_Mask_data = "/content/drive/MyDrive/CNN/COVID 19/Mask"

"""**Installing Tensorflow**"""

# Install any required libraries (if not already installed)
!pip install tensorflow numpy matplotlib opencv-python

"""**Step 1: installing necessary packages and importing the required libraries.**"""

# Importing necessary libraries
import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, UpSampling2D, Concatenate
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import seaborn as sns

# Check TensorFlow version (optional)
print(f"TensorFlow version: {tf.__version__}")

"""**Step 2: Loading and Preprocessing the Images**

Next, we need to load the images and preprocess them.
"""

def load_images(image_dir, mask_dir, target_size=(224, 224)):
    images = []
    masks = []
    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])
    mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])

    for img_file, mask_file in zip(image_files, mask_files):
        img_path = os.path.join(image_dir, img_file)
        mask_path = os.path.join(mask_dir, mask_file)

        # Load image and mask
        img = cv2.imread(img_path)
        img = cv2.resize(img, target_size)
        img = img / 255.0  # Normalize

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.resize(mask, target_size)
        mask = mask / 255.0  # Normalize

        images.append(img)
        masks.append(mask)

    return np.array(images), np.array(masks)

COVID19_data, COVID19_mask_data = load_images(COVID19_data, COVID19_Mask_data)
print(f"Loaded {len(COVID19_data)} images and {len(COVID19_mask_data)} masks.")

# Shuffle the dataset to ensure randomness
COVID19_data, COVID19_mask_data = shuffle(COVID19_data, COVID19_mask_data, random_state=42)

# Manually split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(COVID19_data, COVID19_mask_data, test_size=0.2, random_state=42, stratify=None)

from tensorflow.keras.layers import Conv2D, UpSampling2D

# Load the pre-trained VGG16 model, without the top layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add more layers to ensure the output matches the input size
x = Conv2D(512, (3, 3), activation='relu', padding='same')(base_model.output)
x = UpSampling2D(size=(2, 2))(x)  # Upsampling to 14x14
x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D(size=(2, 2))(x)  # Upsampling to 28x28
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D(size=(2, 2))(x)  # Upsampling to 56x56
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D(size=(2, 2))(x)  # Upsampling to 112x112
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D(size=(2, 2))(x)  # Upsampling to 224x224
predictions = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(x)  # Output mask of same size as input image

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Data augmentation for the training set
train_data_gen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2,
                                    height_shift_range=0.2, horizontal_flip=True)

# No data augmentation for the validation set, just rescaling
val_data_gen = ImageDataGenerator()

# Creating generators
train_generator = train_data_gen.flow(X_train, y_train, batch_size=16)
validation_generator = val_data_gen.flow(X_val, y_val, batch_size=16)

# Debugging check: Validate the data generator
for x, y in validation_generator:
    print(f"Batch x shape: {x.shape}, Batch y shape: {y.shape}")
    break

val_data_gen = ImageDataGenerator()

# Create the validation generator with a proper batch size
validation_generator = val_data_gen.flow(X_val, y_val, batch_size=16)

validation_steps = max(1, len(validation_generator))

# Define the batch size
batch_size = 16

# Calculate the steps per epoch and validation steps
steps_per_epoch = len(X_train) // batch_size
validation_steps = len(X_val) // batch_size

# If there are any remainder samples, ensure they are included in the final step
if len(X_train) % batch_size != 0:
    steps_per_epoch += 1
if len(X_val) % batch_size != 0:
    validation_steps += 1

for epoch in range(10):
    print(f"Epoch {epoch+1}/10")

    # Train step
    model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=1)

    # Validation step
    val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)
    print(f"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}")

# Evaluate the model
val_loss, val_acc = model.evaluate(validation_generator)
print(f"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}")

import matplotlib.pyplot as plt

# Get a batch of validation data
X_val_batch, y_val_batch = next(validation_generator)  # Corrected line

# Predict the masks for this batch
predicted_masks = model.predict(X_val_batch)

# Plot the original image, ground truth mask, and predicted mask
for i in range(5):
    plt.figure(figsize=(10, 10))

    plt.subplot(1, 3, 1)
    plt.imshow(X_val_batch[i])
    plt.title('Original Image')

    plt.subplot(1, 3, 2)
    plt.imshow(y_val_batch[i].reshape(224, 224), cmap='gray')
    plt.title('Ground Truth Mask')

    plt.subplot(1, 3, 3)
    plt.imshow(predicted_masks[i].reshape(224, 224), cmap='gray')
    plt.title('Predicted Mask')

    plt.show()

"""---

**Addressing Algorithmic Bias**

Data Augmentation and Balance:

Issue: Bias in the model can stem from imbalanced datasets, where certain classes (e.g., COVID-19 positive cases) might be underrepresented. Solution: Use data augmentation techniques to artificially increase the diversity of your underrepresented class. Also, consider oversampling or undersampling techniques to balance the dataset.
"""

from sklearn.utils import resample

# Assuming COVID19_data and COVID19_mask_data are numpy arrays
# Separating the minority and majority classes
covid_images = X_train[y_train == 1]
non_covid_images = X_train[y_train == 0]

# Resample the minority class
covid_images_upsampled = resample(covid_images, replace=True, n_samples=len(non_covid_images), random_state=42)

# Combine the resampled dataset
X_train_balanced = np.vstack((non_covid_images, covid_images_upsampled))
y_train_balanced = np.hstack((np.zeros(len(non_covid_images)), np.ones(len(covid_images_upsampled))))

# Shuffle the balanced dataset
X_train_balanced, y_train_balanced = shuffle(X_train_balanced, y_train_balanced, random_state=42)

"""**Model Performance Evaluation Across Subgroups:**

Issue: A model may perform well on the overall dataset but poorly on specific subgroups.

Solution: Evaluate model performance separately for different subgroups, such as COVID-19 vs. non-COVID-19, to identify any biases.
"""

from sklearn.metrics import classification_report

# Predict on the validation set
y_pred = model.predict(validation_generator)

# Since y_pred is continuous, apply a threshold to convert it to binary class labels
y_pred_binary = (y_pred > 0.5).astype(int)

# Flatten the predictions and ground truths to match dimensions
y_pred_binary = y_pred_binary.flatten()
y_val_flat = y_val.flatten()

# Evaluate performance on each subgroup
print("Classification Report for COVID-19 Class:")
print(classification_report(y_val_flat[y_val_flat == 1], y_pred_binary[y_val_flat == 1]))

print("Classification Report for Non-COVID-19 Class:")
print(classification_report(y_val_flat[y_val_flat == 0], y_pred_binary[y_val_flat == 0]))



"""

---

"""

